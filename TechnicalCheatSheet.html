<!DOCTYPE HTML>
<html>

<head>
	<meta charset="utf-8">
	<title>Tech Interview Cheat Sheet</title>
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ"
	 crossorigin="anonymous">
</head>

<body>
	<div id="wrapper">
		<header id="header" class="inner">
		</header>

		<div id="content" class="inner">
			<h2 class="title">Tech Interview Cheat Sheet</h2>
			<div class="entry-content">
				<h2>Data Structures</h2>

				<h3>
					<strong>Array</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Collection of elements (values or variables) consecutevily allocated in memory, each identified by, most commonly 0
						based, index. An array is stored so that the position of each element can be computed from its index tuple by a mathematical
						formula.
					</li>
					<li>Based on
						<a href="http://en.wikipedia.org/wiki/Tuple">tuples</a> from set theory.</li>
					<li>They are one of the oldest, most commonly used data structures.</li>
				</ul>


				<h4>What you need to know:</h4>

				<ul>
					<li>Optimal for indexing; bad at searching, inserting, and deleting (except at the end).</li>
					<li>
						<strong>Static linear arrays</strong>, or one dimensional arrays, are the most basic.

						<ul>
							<li>Are static in size, meaning that they are declared with a fixed size.</li>
						</ul>
					</li>
					<li>
						<strong>Dynamic arrays</strong> are like one dimensional arrays, but have reserved space for additional elements.

						<ul>
							<li>If a dynamic array is full, it copies it&rsquo;s contents to a larger array.</li>
						</ul>
					</li>
					<li>
						<strong>Two dimensional arrays</strong> have x and y indices like a grid or nested arrays.</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Indexing: Linear array: O(1), Dynamic array: O(1)</li>
					<li>Search: Linear array: O(n), Dynamic array: O(n)</li>
					<li>Optimized Search: Linear array: O(log n), Dynamic array: O(log n)</li>
					<li>Insertion: Linear array: n/a Dynamic array: O(n)</li>
				</ul>

				<h3>
					<strong>Linked List</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Node based collection of elements where every node stores data a reference to other nodes.</li>
					<li>Nodes, at its most basic it has one data and one reference (another node). These are called single linked list.</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Designed to optimize insertion and deletion, slow at indexing and searching.</li>
					<li>
						<strong>Doubly linked list</strong> has nodes that reference the previous node.</li>
					<li>
						<strong>Circularly linked list</strong> is simple linked list whose
						<strong>tail</strong>, the last node, references the
						<strong>head</strong>, the first node.</li>
				</ul>


				<h4>Big O efficiency:</h4>

				<table>
					<tr>
						<th>Operation</th>
						<th>Worse</th>
					</tr>
					<tr>
						<td>Indexing</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Search</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Insertion first or last</td>
						<td>O(1)</td>
					</tr>
					<tr>
						<td>Deletion first or last</td>
						<td>O(1)</td>
					</tr>
					<tr>
						<td>Insertion other</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Deletion other</td>
						<td>O(n)</td>
					</tr>
				</table>

				<h2>
					<strong>Stack</strong>
				</h2>

				<ul>
					<li>Commonly implemented with linked lists but can be made from arrays too.</li>
					<li>Stacks are
						<strong>last in, first out</strong> (LIFO) data structures.</li>
					<li>Made with a linked list by having the head be the only place for insertion and removal.</li>
				</ul>


				<h2>
					<strong>Queues </strong>
				</h2>

				<ul>
					<li>It can be implemented with a linked list or an array.</li>
					<li>Queues are a
						<strong>first in, first out</strong> (FIFO) data structure.</li>
					<li>Made with a doubly linked list that only removes from head and adds to tail.</li>
				</ul>


				<h3>
					<strong>Binary Tree</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>It is a tree like data structure where every node has at most two children.

						<ul>
							<li>There is one left and right child node.</li>
						</ul>
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Designed to optimize searching and sorting.</li>
					<li>A
						<strong>degenerate tree</strong> is an unbalanced tree, which if entirely one-sided is a essentially a linked list.</li>
					<li>Used to make
						<strong>binary search trees</strong>.

						<ul>
							<li>A binary tree that uses comparable keys to assign which direction a child is.</li>
							<li>Left child has a key smaller than it&rsquo;s parent node.</li>
							<li>Right child has a key greater than it&rsquo;s parent node.</li>
							<li>There can be no duplicate node.</li>
							<li>Tree traversal can be done in preorder, inorder and postorder</li>
							<li>It is usually self-balanced. Automatically keeps its height (maximal number of levels below the root) small in the
								face of arbitrary item insertions and deletions.</li>
							<li>Because of the above it is more likely to be used as a data structure than a binary tree.</li>
						</ul>
					</li>
				</ul>
				<h4>Big O efficiency:</h4>

				<table>
					<tr>
						<th>Operation</th>
						<th>Average (No rebalance needed)</th>
						<th>Worse (Degenerated tree to linkedlist)</th>
					</tr>
					<tr>
						<td>Indexing</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Search</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Insertion</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Deletion</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
				</table>

				<h3>
					<strong>Heap</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Tree-based data structure that satisfies the heap property: if P is a parent node of C, then the key (the value) of
						P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>The heap is one maximally efficient implementation of an abstract data type called a priority queue</li>
					<li>There is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there
						would be in, e.g., a binary search tree)..</li>
					<li>Heaps where the parent key is greater than or equal to (≥) the child keys are called max-heaps; those where it is less
						than or equal to (≤) are called min-heaps.</li>
					<li>Binary heaps are also commonly employed in the heapsort sorting algorithm, which is an in-place algorithm owing to the
						fact that binary heaps can be implemented as an implicit data structure, storing keys in an array and using their relative
						positions within that array to represent child-parent relationships.</li>
					<li>Used to implement priority queues</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<table>
					<tr>
						<th>Operation</th>
						<th>Average</th>
						<th>Worse (Degenerated tree to linkedlist)</th>
					</tr>
					<tr>
						<td>Indexing</td>
						<td>O(n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Search</td>
						<td>O(n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Search min</td>
						<td>O(1)</td>
						<td>O(1)</td>
					</tr>
					<tr>
						<td>Insertion</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Deletion</td>
						<td>O(log n)</td>
						<td>O(n)</td>
					</tr>
				</table>

				<h3>
					<strong>B-Tree</strong>
				</h3>

				<h4>Definition:</h4>
				<ul>
					<li>It is a tree like data structure where every node has at most two children.</li>
				</ul>

				<h3>
					<strong>Hash Table</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Stores data with key value pairs.</li>
					<li>
						<strong>Hash functions</strong> accept a key and return an output unique only to that specific key.

						<ul>
							<li>This is known as
								<strong>hashing</strong>, which is the concept that an input and an output have a one-to-one correspondence to map information.</li>
							<li>Hash functions return a unique address in memory for that data.</li>
						</ul>
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Designed to optimize searching, insertion, and deletion.</li>
					<li>
						<strong>Hash collisions</strong> are when a hash function returns the same output for two distinct outputs.

						<ul>
							<li>All hash functions have this problem.</li>
							<li>This is often accommodated for by having the hash tables be very large.</li>
						</ul>
					</li>
					<li>Hashes are important for associative arrays and database indexing.</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<table>
					<tr>
						<th></th>
						<th>Average</th>
						<th>Worse (Degenerated buckets in the hash due to collision)</th>
					</tr>
					<tr>
						<td>Indexing</td>
						<td>O(1)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Search</td>
						<td>O(1)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Insertion</td>
						<td>O(1)</td>
						<td>O(n)</td>
					</tr>
					<tr>
						<td>Deletion</td>
						<td>O(1)</td>
						<td>O(n)</td>
					</tr>
				</table>

				<h3>
					<strong>Graph</strong>
				</h3>
				<h4>Definition:</h4>
				<ul>
					<li>
						A graph data structure consists of a finite set of nodes, together with a set of unordered pairs of these nodes for an undirected
						graph or a set of ordered pairs for a directed graph. These pairs are known as edges for an undirected graph and as
						directed edges for a directed graph.
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>It can be implemented using an adjacency list or a matrix.</li>
				</ul>

				<h4>Big O efficiency:</h4>
				<table>
					<tr>
						<th></th>
						<th>Adjacency list</th>
						<th>Adjacency matrix</th>
					</tr>
					<tr>
						<td>Store graph</td>
						<td>O(|V|+|E|)</td>
						<td>O(|V|^{2})</td>
					</tr>
					<tr>
						<td>Add vertex</td>
						<td>O(1)</td>
						<td>O(|V|^{2})</td>
					</tr>
					<tr>
						<td>Add edge</td>
						<td>O(1)</td>
						<td>O(1)</td>
					</tr>
					<tr>
						<td>Remove vertex</td>
						<td>O(|E|)</td>
						<td>O(|V|^{2})</td>
					</tr>
					<tr>
						<td>Remove edge</td>
						<td> O(|V|)</td>
						<td>O(1)</td>
					</tr>
					<tr>
						<td>Query: are vertices x and y adjacent? (assuming that their storage positions are known)</td>
						<td>O(|V|)</td>
						<td>O(1)</td>
					</tr>
				</table>

				<h2>Search Basics</h2>

				<h3>
					<strong>Breadth First Search</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>An algorithm that searches a tree (or graph) by searching levels of the tree first, starting at the root.

						<ul>
							<li>It finds every node on the same level, most often moving left to right.</li>
							<li>While doing this it tracks the children nodes of the nodes on the current level.</li>
							<li>When finished examining a level it moves to the left most node on the next level.</li>
							<li>The bottom-right most node is evaluated last (the node that is deepest and is farthest right of it&rsquo;s level).</li>
						</ul>
					</li>
				</ul>


				<h4>What you need to know:</h4>

				<ul>
					<li>Optimal for searching a tree that is wider than it is deep.</li>
					<li>Uses a queue to store information about the tree while it traverses a tree.
						<ul>
							<li>Because it uses a queue it is more memory intensive than
								<strong>depth first search</strong>.</li>
							<li>The queue uses more memory because it needs to stores pointers</li>
						</ul>
					</li>
				</ul>


				<h4>Big O efficiency:</h4>

				<ul>
					<li>Search: Breadth First Search: O(|E| + |V|)</li>
					<li>E is number of edges</li>
					<li>V is number of vertices</li>
				</ul>

				<h3>
					<strong>Depth First Search</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>An algorithm that searches a tree (or graph) by searching depth of the tree first, starting at the root.

						<ul>
							<li>It traverses left down a tree until it cannot go further.</li>
							<li>Once it reaches the end of a branch it traverses back up trying the right child of nodes on that branch, and if possible
								left from the right children.</li>
							<li>When finished examining a branch it moves to the node right of the root then tries to go left on all it&rsquo;s children
								until it reaches the bottom.</li>
							<li>The right most node is evaluated last (the node that is right of all it&rsquo;s ancestors).</li>
						</ul>
					</li>
				</ul>


				<h4>What you need to know:</h4>

				<ul>
					<li>Optimal for searching a tree that is deeper than it is wide.</li>
					<li>Uses a stack to push nodes onto.

						<ul>
							<li>Because a stack is LIFO it does not need to keep track of the nodes pointers and is therefore less memory intensive
								than breadth first search.</li>
							<li>Once it cannot go further left it begins evaluating the stack.</li>
						</ul>
					</li>
				</ul>


				<h4>Big O efficiency:</h4>

				<ul>
					<li>Search: Depth First Search: O(|E| + |V|)</li>
					<li>E is number of edges</li>
					<li>V is number of vertices</li>
				</ul>


				<h4>Breadth First Search Vs. Depth First Search</h4>

				<ul>
					<li>The simple answer to this question is that it depends on the size and shape of the tree.

						<ul>
							<li>For wide, shallow trees use Breadth First Search</li>
							<li>For deep, narrow trees use Depth First Search</li>
						</ul>
					</li>
				</ul>

				<h4>Nuances:</h4>

				<ul>
					<li>Because BFS uses queues to store information about the nodes and its children, it could use more memory than is available
						on your computer. (But you probably won&rsquo;t have to worry about this.)</li>
					<li>If using a DFS on a tree that is very deep you might go unnecessarily deep in the search. See
						<a href="http://xkcd.com/761/">xkcd</a> for more information.</li>
					<li>Breadth First Search tends to be a looping algorithm.</li>
					<li>Depth First Search tends to be a recursive algorithm.</li>
				</ul>

				<h2>Comparison sort algorithms</h2>

				<h3>
					<strong>Bubble Sort</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>It iterates left to right comparing every couple, moving the smaller element to the left.</li>
					<li>It repeats this process until it no longer moves and element to the left.</li>
				</ul>


				<h4>What you need to know:</h4>

				<ul>
					<li>While it is very simple to implement, it is the least efficient of the sorting methods.</li>
					<li>Know that it moves one space to the right comparing two elements at a time and moving the smaller on to left.</li>
					<li>Can be optimized to avoid up to 50% of the comparisons by starting from the last swap in the previous iteration</li>
					<li>Stable</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n)</li>
					<li>Average Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Worst Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Space: O(1)</li>
				</ul>

				<h3>
					<strong>Insertion Sort</strong>
				</h3>

				<h4>Definition:</h4>


				<ul>
					<li>Builds the final sorted array (or list) one item at a time.</li>
					<li>At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the
						sorted list, and inserts it there. It repeats until no input elements remain.</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Efficient for (quite) small data sets, much like other quadratic sorting algorithms such as selection sort or bubble
						sort
					</li>
					<li>More efficient in practice than most other simple quadratic algorithms.</li>
					<li>Stable</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Average Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Worst Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Space: O(1)</li>
				</ul>
				<h3>
					<strong>Selection Sort</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left
						to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the
						list.
					</li>
					<li>Initially, the sorted sublist is empty and the unsorted sublist is the entire input list.</li>
					<li>The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist,
						exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries
						one element to the right </li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Generally performs worse than the similar insertion sort.</li>
					<li>Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain
						situations, particularly where auxiliary memory is limited.</li>
					<li>Unstable. Stable implementation can be done with additional data structure.</li>
				</ul>


				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Average Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Worst Case Sort: O(n
						<sup>2</sup>)</li>
					<li>Space: O(1)</li>
				</ul>

				<h3>
					<strong>Merge Sort</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Divide the unsorted list into n sublists, each containing 1 element (a list of 1 element is considered sorted).</li>
					<li>Repeatedly merge sublists to produce new sorted sublists until there is only 1 sublist remaining. This will be the sorted
						list.
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Implementation of the divide and conquer algorithm stategy.</li>
					<li>Know that it divides all the data into as small possible sets then compares them.</li>
					<li>Most of the implementations stable sort</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n)</li>
					<li>Average Case Sort: O(n log n)</li>
					<li>Worst Case Sort: O(n log n)</li>
				</ul>

				<h3>
					<strong>Quicksort</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>Pick an element, called a pivot, from the array.</li>
					<li>Partitioning: reorder the array so that all elements with values less than the pivot come before the pivot, while all
						elements with values greater than the pivot come after it (equal values can go either way). After this partitioning,
						the pivot is in its final position. This is called the partition operation.</li>
					<li>Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of
						elements with greater values.</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>While it has the same Big O as (or worse in some cases) many other sorting algorithms it is often faster in practice
						than many other sorting algorithms, such as merge sort.</li>
					<li>Know that it halves the data set by the average continuously until all the information is sorted.</li>
					<li>Unstable</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n)</li>
					<li>Average Case Sort: O(n log n)</li>
					<li>Worst Case Sort: O(n
						<sup>2</sup>)</li>
				</ul>

				<h4>Merge Sort vs Quicksort</h4>
				<ul>
					<li>Quicksort is likely faster in practice.</li>
					<li>Merge Sort divides the set into the smallest possible groups immediately then reconstructs the incrementally as it sorts
						the groupings.</li>
					<li>Quicksort continually divides the set by the average, until the set is recursively sorted.</li>
				</ul>

				<h4>Merge Sort vs Heapsort</h4>
				<ul>

					<li> Merge sort requires O(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster
						in practice on machines with small or slow data caches, and does not require as much external memory.</li>
					<li>Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop
						computers because merge sort frequently accesses contiguous memory locations (good locality of reference); heapsort
						references are spread throughout the heap.</li>
					<li>Heapsort is not a stable sort; merge sort is stable.</li>
					<li>Merge sort parallelizes well and can achieve close to linear speedup with a trivial implementation; heapsort is not
						an obvious candidate for a parallel algorithm.</li>
					<li>Merge sort can be adapted to operate on singly linked lists with O(1) extra space. Heapsort can be adapted to operate
						on doubly linked lists with only O(1) extra space overhead.</li>
					<li>Merge sort is used in external sorting; heapsort is not. Locality of reference is the issue.</li>
				</ul>

				<h3>
					<strong>Heap Sort</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>A comparison based sorting algorithm

						<ul>
							<li>In the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete
								binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents
								a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based
								array, the root node is stored at index 0; if i is the index of the current node, then
							</li>
							<li> In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of
								the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property.
								Once all objects have been removed from the heap, the result is a sorted array.
							</li>
							<li>Parent(i) = floor((i-1) / 2) where floor functions map a real number to the smallest leading integer. iLeftChild(i)
								= 2*i + 1 iRightChild(i) = 2*i + 2</li>
						</ul>
					</li>
				</ul>
				<h4>What you need to know:</h4>

				<ul>
					<li>Heapsort can be thought of as an improved selection sort.</li>
					<li>Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more
						favorable worst-case O(n log n) runtime</li>
					<li>Not a stable sort</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Best Case Sort: O(n log n) O(n) if equal keys</li>
					<li>Average Case Sort: O(n log n),</li>
					<li>Worst Case Sort: O(n log n)</li>
					<li>Space: O(1)</li>
				</ul>

				<h2>Non comparison sort algorithms</h2>

				<h3>
					<strong>Pigeonhole Sort</strong>
				</h3>
				<h4>Definition:</h4>
				<ul>
					<li>Given an array of values to be sorted, set up an auxiliary array of initially empty "pigeonholes," one pigeonhole for
						each key through the range of the original array.</li>
					<li>Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole
						eventually contains a list of all values with that key.</li>
					<li>Iterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key
						values (N) are approximately the same.</li>
					<li>Stable</li>
				</ul>

				<h4>Big O efficiency:</h4>

				<ul>
					<li>Worst Case Sort: O(N+n) where N is the range of key values and n is the input size</li>
					<li>Space: O(N+n)</li>
				</ul>


				<h2>Basic Types of Algorithms</h2>

				<h3>
					Asymptotic computational
				</h3>
				<p>Asymptotic computational complexity is the usage of asymptotic analysis (is a method of describing limiting behavior)
					for the estimation of computational complexity of algorithms and computational problems, commonly associated with the
					usage of the big O notation.
				</p>

				<h3>
					<strong>Recursive Algorithms</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>An algorithm that calls itself in its definition.

						<ul>
							<li>
								<strong>Recursive case</strong> a conditional statement that is used to trigger the recursion.</li>
							<li>
								<strong>Base case</strong> a conditional statement that is used to break the recursion.</li>
						</ul>
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>
						<strong>Stack level too deep</strong> and
						<strong>stack overflow</strong>.

						<ul>
							<li>It means that your base case was never triggered because it was faulty or the problem was so massive you ran out of
								RAM before reaching it.</li>
							<li>Knowing whether or not you will reach a base case is integral to correctly using recursion.</li>
							<li>Often used in Depth First Search</li>
						</ul>
					</li>
					<li>
						<strong>Tail recursion and tail call elimination</strong>
					</li>
				</ul>

				<h3>
					<strong>Iterative Algorithms</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>An algorithm that is called repeatedly but for a finite number of times, each time being a single iteration.</li>
					<li>Often used to move incrementally through a data set.</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Generally you will see iteration as loops, for, while, and until statements.</li>
					<li>Think of iteration as moving one at a time through a set.</li>
					<li>Often used to move through an array.</li>
				</ul>

				<h4>Recursion Vs. Iteration</h4>

				<ul>
					<li>The differences between recursion and iteration can be confusing to distinguish since both can be used to implement
						the other. But know that,

						<ul>
							<li>Recursion is, usually, more expressive and easier to implement.</li>
							<li>Iteration uses less memory.</li>
						</ul>
					</li>
					<li>
						<strong>Functional languages</strong> tend to use recursion. (i.e. Haskell)</li>
					<li>
						<strong>Imperative languages</strong> tend to use iteration. (i.e. Ruby)</li>
					<li>
						Tail recursion. Tail call elimination.Tail calls can be implemented without adding a new stack frame to the call stack. Most
						of the frame of the current procedure is no longer needed, and can be replaced by the frame of the tail cal </li>
					<li>Check out this
						<a href="http://stackoverflow.com/questions/19794739/what-is-the-difference-between-iteration-and-recursion">Stack Overflow post</a> for more info.</li>
				</ul>

				<h4>Pseudo Code of Moving Through an Array (this is why iteration is used for this)</h4>

				<figure class='code'>
					<div class="highlight">
						<table>
							<tr>
								<td class="gutter">
									<pre class="line-numbers"><span class='line-number'>1</span>
										<span class='line-number'>2</span>
										<span class='line-number'>3</span>
										<span class='line-number'>4</span>
										<meta name="description" content="Studying for a Tech Interview Sucks, so Here&rsquo;s a Cheat Sheet to Help This list is meant to be a both a quick guide and reference for further &hellip;">
									
										<span class='line-number'>5</span>
										<span class='line-number'>6</span>
										<span class='line-number'>7</span>
										<span class='line-number'>8</span>
										</pre>
								</td>
								<td class='code'>
									<pre><code class=''><span class='line'>Recursion                         | Iteration
															</span><span class='line'>----------------------------------|----------------------------------
															</span><span class='line'>recursive method (array, n)       | iterative method (array)
															</span><span class='line'>  if array[n] is not nil          |   for n from 0 to size of array
															</span><span class='line'>    print array[n]                |     print(array[n])
															</span><span class='line'>    recursive method(array, n+1)  |
															</span><span class='line'>  else                            |
															</span><span class='line'>    exit loop                     |</span></code></pre>
								</td>
							</tr>
						</table>
					</div>
				</figure>

				<h3>
					<strong>Greedy Algorithm</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>An algorithm that, while executing, selects only the information that meets a certain criteria.</li>
					<li>The general five components, taken from
						<a href="http://en.wikipedia.org/wiki/Greedy_algorithm#Specifics">Wikipedia</a>:

						<ul>
							<li>A candidate set, from which a solution is created.</li>
							<li>A selection function, which chooses the best candidate to be added to the solution.</li>
							<li>A feasibility function, that is used to determine if a candidate can be used to contribute to a solution.</li>
							<li>An objective function, which assigns a value to a solution, or a partial solution.</li>
							<li>A solution function, which will indicate when we have discovered a complete solution.</li>
						</ul>
					</li>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Used to find the optimal solution for a given problem.</li>
					<li>Generally used on sets of data where only a small proportion of the information evaluated meets the desired result.</li>
					<li>Often a greedy algorithm can help to reduce the Big O of an algorithm.</li>
				</ul>

				<h4>Pseudo Code of a Greedy Algorithm to Find Largest Difference of any Two Numbers in an Array.</h4>

				<figure class='code'>
					<div class="highlight">
						<table>
							<tr>
								<td class="gutter">
									<pre class="line-numbers">
										<span class='line-number'>1</span>
										<span class='line-number'>2</span>
										<span class='line-number'>3</span>
										<span class='line-number'>4</span>
										<span class='line-number'>5</span>
										<span class='line-number'>6</span>
									</pre>
								</td>
								<td class='code'>
									<code class=''>
										<span class='line'>greedy algorithm (array)</span>
										<span class='line'>  var largest difference = 0</span>
										<span class='line'>  var new difference = find next difference (array[n], array[n+1])</span>
										<span class='line'>  largest difference = new difference if new difference is &gt; largest difference</span>
										<span class='line'>  repeat above two steps until all differences have been found</span>
										<span class='line'>  return largest difference</span>
									</code>
								</td>
							</tr>
						</table>
					</div>
				</figure>

				<p>This algorithm never needed to compare all the differences to one another, saving it an entire iteration.</p>


				<h3>
					<strong>Dynamic Programming</strong>
				</h3>

				<h4>Definition:</h4>

				<ul>
					<li>It is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each
						of those subproblems just once, and storing their solutions. </li>

					<li> The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed
						solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space.</li>
					<ul>
						<li>Find the recursion in the problem.</li>
						<li>Top-down: store the answer for each subproblem in a table to avoid having to recompute them.</li>
						<li>Bottom-up: Find the right order to evaluate the results so that partial results are available when needed.</li>
					</ul>
				</ul>

				<h4>What you need to know:</h4>

				<ul>
					<li>Memoization is a term describing an optimization technique where you cache previously computed results, and return the
						cached result when the same computation is needed again. </li>
					<li>Dynamic programming is typically implemented using tabulation, but can also be implemented using memoization.</li>
					<li>When using tabulation you solve the problem "bottom up", i.e., by solving all related sub-problems first, typically
						by filling up an n-dimensional table. Based on the results in the table, the solution to the "top" / original problem
						is then computed.</li>
					<li> When using memoization, a map of already solved sub problems is maintained. You do it "top down" in the sense that you
						solve the "top" problem first (which typically recurses down to solve the sub-problems).</li>
				</ul>

				<h2>Polymorphism</h2>
				<p>From type theory is the provision of a single interface to entities of different types</p>
				<h3>Types</h3>
				<ul>
					<li>
						<strong>Ad hoc polymorphism:</strong> when a function denotes different and potentially heterogeneous implementations depending
						on a limited range of individually specified types and combinations. Ad hoc polymorphism is supported in many languages
						using function overloading.</li>
					<li>
						<strong>Parametric polymorphism:</strong> when code is written without mention of any specific type and thus can be used transparently
						with any number of new types. In the object-oriented programming community, this is often known as generics or generic
						programming. In the functional programming community, this is often shortened to polymorphism.</li>
					<li>
						<strong>Subtyping (also called subtype polymorphism or inclusion polymorphism):</strong> when a name denotes instances of many
						different classes related by some common superclass.
					</li>
				</ul>
				<h3>Implementations</h3>
				<p>Polymorphism can be distinguished by when the implementation is selected: statically (at compile time) or dynamically
					(at run time, typically via a virtual function). This is known respectively as static dispatch and dynamic dispatch,
					and the corresponding forms of polymorphism are accordingly called static polymorphism and dynamic polymorphism.</p>
				<ul>
					<li>Static polymorphism executes faster, because there is no dynamic dispatch overhead, but requires additional compiler
						support. Further, static polymorphism allows greater static analysis by compilers (notably for optimization), source
						code analysis tools, and human readers (programmers).</li>
					<li>Dynamic polymorphism is more flexible but slower—for example, dynamic polymorphism allows duck typing, and a dynamically
						linked library may operate on objects without knowing their full type.</li>
					<li>Static polymorphism typically occursy in ad hoc polymorphism and parametric polymorphism, whereas dynamic polymorphism
						is usual for subtype polymorphism. However, it is possible to achieve static polymorphism with subtyping through more
						sophisticated use of template metaprogramming, namely the curiously recurring template pattern.</li>

					<li>Examples static: templates/generics, function overloading, macros</li>
					<li>Examples dynamic: duck typing, dynamic dispatch with vtables (subtyping) </li>
				</ul>

				<h3>Duck typing for polymorphism without (static) types</h3>
				<ul>
					<li>Duck typing is concerned with establishing the suitability of an object for some purpose, using the principle, "If it
						walks like a duck and it quacks like a duck, then it must be a duck." With normal typing, suitability is assumed to
						be determined by an object's type only. In duck typing, an object's suitability is determined by the presence of certain
						methods and properties (with appropriate meaning), rather than the actual type of the object.</li>
				</ul>

				<h2>Testing</h2>

				<h2>Agile software development</h2>
				<ul>
					<li>TDD</li>
					<li>BDD</li>
					<li>SCRUM</li>
				</ul>

				<h2>Design Patterns</h2>
				<ul>
					<li>Behavioral</li>
					<li>Structural</li>
					<li>Creational</li>
					<li>Concurrency</li>
				</ul>

				<h2>SOLID principles</h2>
				<ul>
					<li>Single responsability principle</li>
					<li>Open/close principle</li>
					<li>Liskov substituion principle</li>
					<li>Interface segregation principle</li>
					<li>Dependency inversion principle</li>
					<li>bonus: YAGNI, KISS</li>
				</ul>
				<h2>Design smells</h2>
				<ul>
					<li>Rigidity is the tendency of software to be difficult to change. </li>
					<li>Fragility is the tendency of software to break in many places when a single change is made. </li>
					<li>Immobility A design is immobile when it contains parts that could be useful in other systems, but the effort and risk
						of separating them from the original system are too great.
					</li>
					<li>Viscosity When faced with a change, developers often have several ways to implement the desired change.</li>
					<li>Needless repetition In short copy and pasting code throughout the system. </li>
					<li>Opacity Opacity is the tendency of a module to be difficult to understand. </li>
					<li>Needless complexity This may very well be the most important smell of bad design. </li>
				</ul>

				<h2>Network</h2>

				<h3>
					<strong>Internet protocol suite - TCP/IP</strong>
				</h3>
				<p>
					The Internet protocol suite provides end-to-end data communication specifying how data should be packetized, addressed, transmitted,
					routed, and received. This functionality is organized into four abstraction layers which classify all related protocols
					according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication
					methods for data that remains within a single network segment (link); the internet layer, providing internetworking
					between independent networks; the transport layer handling host-to-host communication; and the application layer, which
					provides process-to-process data exchange for applications.
				</p>

				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/IP_stack_connections.svg/490px-IP_stack_connections.svg.png"
				 class="svg">
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/UDP_encapsulation.svg/800px-UDP_encapsulation.svg.png"
				 class="svg">
				<ul>
					<li> Application layer. The application layer is the scope within which applications create user data and communicate this
						data to other applications on another or the same host. The applications, or processes, make use of the services provided
						by the underlying, lower layers, especially the Transport Layer which provides reliable or unreliable pipes to other
						processes. The communications partners are characterized by the application architecture, such as the client-server
						model and peer-to-peer networking. This is the layer in which all higher level protocols, such as SMTP, FTP, SSH, HTTP,
						operate. Processes are addressed via ports which essentially represent services.
					</li>
					<li>Transport layer. The transport layer performs host-to-host communications on either the same or different hosts and
						on either the local network or remote networks separated by routers. It provides a channel for the communication needs
						of applications. UDP is the basic transport layer protocol, providing an unreliable datagram service. The Transmission
						Control Protocol provides flow-control, connection establishment, and reliable transmission of data.
					</li>
					<li>Internet layer. The internet layer exchanges datagrams across network boundaries. It provides a uniform networking interface
						that hides the actual topology (layout) of the underlying network connections. It is therefore also referred to as
						the layer that establishes internetworking, indeed, it defines and establishes the Internet. This layer defines the
						addressing and routing structures used for the TCP/IP protocol suite. The primary protocol in this scope is the Internet
						Protocol, which defines IP addresses. Its function in routing is to transport datagrams to the next IP router that
						has the connectivity to a network closer to the final data destination.
					</li>
					<li>Link layer. The link layer defines the networking methods within the scope of the local network link on which hosts
						communicate without intervening routers. This layer includes the protocols used to describe the local network topology
						and the interfaces needed to effect transmission of Internet layer datagrams to next-neighbor hosts.
					</li>
				</ul>
				<h3>
					<strong>Ethernet</strong>
				</h3>

				<h3>
					<strong>TCP</strong>
				</h3>
				<p>
					TCP provides reliable, ordered, and error-checked delivery of a stream of octets (bytes) between applications running on
					hosts communicating by an IP network.
				</p>

				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Tcp_state_diagram_fixed_new.svg/1194px-Tcp_state_diagram_fixed_new.svg.png"
				 width="796" height="600" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Tcp_state_diagram_fixed_new.svg/1194px-Tcp_state_diagram_fixed_new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Tcp_state_diagram_fixed_new.svg/1592px-Tcp_state_diagram_fixed_new.svg.png 2x"
				 data-file-width="1084" data-file-height="817">

				<ul>
					<li>CLOSED --> LISTEN -- sync/sync ack --> SYNC RECEIVED -- ack/- --> ESTABLISHED </li>
					<li>ESTABLISHED -- close/fin --> FIN WAIT 1 --fin/ack--> CLOSING -ack/--> CLOSED</li>
					<li>Package format</li>
					<li>Symetric vs asymetric conexion</li>
				</ul>

				<h3>
					<strong>IP</strong>
				</h3>

				<ul>
					<li>How to start a session</li>
					<li>Package format</li>
					<li>Symetric vs asymetric conexion</li>
				</ul>

				<h3>
					<strong>HTTP</strong>
				</h3>
				<p>
					HTTP is an application protocol for transfer hypertext designed within the framework of the Internet protocol suite. Its
					definition presumes an underlying and reliable transport layer protocol,[4] and Transmission Control Protocol (TCP)
					is commonly used. It functions as a request–response protocol in the client–server computing model. By definition it
					is a stateless transaction.
				</p>

				<h4>Cookies</h4>
				<p>Cookies are arbitrary pieces of data, usually chosen and first sent by the web server, and stored on the client computer
					by the web browser. The browser then sends them back to the server with every request, introducing states (memory of
					previous events) into otherwise stateless HTTP transactions. Without cookies, each retrieval of a web page or component
					of a web page would be an isolated event, largely unrelated to all other page views made by the user on the website.
					Although cookies are usually set by the web server, they can also be set by the client using a scripting language such
					as JavaScript (unless the cookie's HttpOnly flag is set, in which case the cookie cannot be modified by scripting languages).</p>
				<h4>Some common request methods</h4>
				<ul>
					<li>GET: The GET method requests a representation of the specified resource.</li>
					<li>HEAD: The HEAD method asks for a response identical to that of a GET request, but without the response body. This is
						useful for retrieving meta-information written in response headers, without having to transport the entire content.</li>
					<li>DELETE: The DELETE method deletes the specified resource.</li>
					<li>POST: The POST method requests that the server accept the entity enclosed in the request as a new subordinate of the
						web resource identified by the URI. The data POSTed might be, for example, an annotation for existing resources; a
						message for a bulletin board, newsgroup, mailing list, or comment thread; a block of data that is the result of submitting
						a web form to a data-handling process; or an item to add to a database.</li>
					<li>PUT:The PUT method requests that the enclosed entity be stored under the supplied URI. If the URI refers to an already
						existing resource, it is modified; if the URI does not point to an existing resource, then the server can create the
						resource with that URI.</li>
				</ul>

				<h4>Message format</h4>
				<h4>Request</h4>
				<ul>
					<li> A request line (e.g., GET /images/logo.png HTTP/1.1, which requests a resource called /images/logo.png from the server).</li>
					<li>Request header fields (e.g., Accept-Language: en).</li>
					<li>An empty line.</li>
					<li>An optional message body.</li>
				</ul>
				<h4>Response</h4>
				<ul>
					<li> A status line which includes the status code and reason message (e.g., HTTP/1.1 200 OK, which indicates that the client's
						request succeeded).
					</li>
					<li>Response header fields (e.g., Content-Type: text/html).</li>
					<li>An empty line.</li>
					<li>An optional message body.</li>
				</ul>
				<p class="code">
					Client request[edit] GET /index.html HTTP/1.1 Host: www.example.com Server response[edit] HTTP/1.1 200 OK Date: Mon, 23 May
					2005 22:38:34 GMT Content-Type: text/html; charset=UTF-8 Content-Encoding: UTF-8 Content-Length: 138 Last-Modified:
					Wed, 08 Jan 2003 23:11:55 GMT Server: Apache/1.3.3.7 (Unix) (Red-Hat/Linux) ETag: "3f80f-1b6-3e1cb03b" Accept-Ranges:
					bytes Connection: close HTML CONTENT
				</p>

				<h3>
					<strong>HTTPS</strong>
				</h3>
				<p>
					Encrypted HTTP
				</p>
				<h2>Criptography</h2>
				<ul>
					<li>Asymetric or public key encryption</li>
					<ul>
						<li> Asymmetric encryption, also known as public-key encryption, uses two keys, a public key for encryption and a corresponding
							private key for decryption. The public key and private key are mathematically related so that when the public key
							is used for encryption, the corresponding private key must be used for decryption. Two usages.
						</li>
						<li>Public key encryption, in which a message is encrypted with a recipient's public key. The message cannot be decrypted
							by anyone who does not possess the matching private key, who is thus presumed to be the owner of that key and the
							person associated with the public keyx.
						</li>
						<li>Digital signatures, in which a message is signed with the sender's private key and can be verified by anyone who has
							access to the sender's public key. This verification proves that the sender had access to the private key, and therefore
							is likely to be the person associated with the public key. This also ensures that the message has not been tampered
							with, as a signature is mathematically bound to the message it originally was made with, and verification will fail
							for practically any other message, no matter how similar to the original message.
						</li>
					</ul>
					<li>Symetric encryption</li>

					<p>Symmetric encryption uses the same secret key to perform both the encryption and decryption processes.This requirement
						that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison
						to public-key encryption (also known as asymmetric key encryption).</p>

					<li>Symetric encryption vs asymetric encryption</li>
					<p>Choosing between symmetric and asymmetric encryption depends on the use case. Symmetric encryption is used to share
						information between a set of people that all shall have access to it. Furthermore symmetric encryption is nice because
						it is easier to understand (less likely to mess it up) and the algorithms tend to be faster. Asymmetric encryption
						is used when a large number of subsets of people shall be able to share information. Furthermore asymmetric cryptography
						can be used in reverse to sign documents. This is especially interesting because it allows people to certify that a
						public key belongs to a certain person.</p>
					<p>The disadvantage of symmetric encryption Symmetric encryption always use the same key for encryption and decryption
						- that is the very definition of it. That has one major downside. If the person doing the encryption and the decryption
						are not the same, they have to somehow securely share the key. If A generates a random key and encrypts a message for
						B with it, how does he get the key to B? To do this securely, he has to transmit the key out of bound, or encrypt it
						with B's public key using asymmetric encryption. Obviously asymmetric encryption does not suffer from this disadvantage,
						since B can freely share his public key with anybody without loosing any confidentiality.
					</p>
					<p>The disadvantage of asymmetric encryption One word: Performance. It is slower than symmetric encryption. Therefore it
						is in general just used to encrypt a symmetric key that is used to encrypt the rest of the message.</p>

			</div>
		</div>
	</div>
	<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n"
	 crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb"
	 crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn"
	 crossorigin="anonymous"></script>
</body>

</html>